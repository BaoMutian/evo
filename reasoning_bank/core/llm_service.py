"""
LLM æœåŠ¡æ¨¡å—

å°è£… OpenRouter API è°ƒç”¨ï¼Œæ”¯æŒåŒæ­¥å’Œå¼‚æ­¥è°ƒç”¨
"""

import os
import re
import time
import asyncio
from typing import Optional, List, Dict, Any, Union, Tuple
from dataclasses import dataclass

from openai import OpenAI, AsyncOpenAI
from dotenv import load_dotenv

from reasoning_bank.utils.config import get_config
from reasoning_bank.utils.logger import get_logger

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

logger = get_logger("llm_service")


@dataclass
class LLMResponse:
    """LLM å“åº”æ•°æ®ç±»"""
    status: str  # success, failed
    content: Optional[str] = None
    reasoning: Optional[str] = None  # æ¨ç†è¿‡ç¨‹ï¼ˆéƒ¨åˆ†æ¨¡å‹æ”¯æŒï¼‰
    usage: Optional[Dict] = None
    time_taken: float = 0.0
    error: Optional[str] = None


class LLMService:
    """LLM æœåŠ¡ç±»ï¼Œå°è£… OpenRouter API"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        api_base: Optional[str] = None,
        model: Optional[str] = None,
        temperature: float = 0.3,
        max_tokens: int = 4096,
        timeout: int = 120,
        max_retries: int = 3,
        debug: bool = False,
        enable_thinking: Optional[bool] = None,
    ):
        """åˆå§‹åŒ– LLM æœåŠ¡

        Args:
            api_key: API å¯†é’¥ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®è¯»å–
            api_base: API åŸºç¡€åœ°å€
            model: é»˜è®¤æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            debug: æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼ï¼ˆæ‰“å°å®Œæ•´ prompt å’Œå“åº”ï¼‰
            enable_thinking: Qwen3 æ€è€ƒæ¨¡å¼å¼€å…³ï¼ˆNone=ä¸ä¼ é€’ä½¿ç”¨æ¨¡å‹é»˜è®¤ï¼ŒTrue=å¼€å¯ï¼ŒFalse=å…³é—­ï¼‰
        """
        # ä»é…ç½®æˆ–ç¯å¢ƒå˜é‡è·å–å‚æ•°
        self.api_key = api_key or os.getenv(
            "OPENROUTER_API_KEY") or get_config("llm.api_key")
        self.api_base = api_base or os.getenv("OPENROUTER_API_BASE") or get_config(
            "llm.api_base", "https://openrouter.ai/api/v1")
        self.default_model = model or get_config(
            "llm.default_model", "qwen/qwen-2.5-7b-instruct")
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.max_retries = max_retries
        self.debug = debug
        self.enable_thinking = enable_thinking

        if not self.api_key:
            raise ValueError("API Key æœªè®¾ç½®ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENROUTER_API_KEY æˆ–åœ¨é…ç½®ä¸­æŒ‡å®š")

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._sync_client: Optional[OpenAI] = None
        self._async_client: Optional[AsyncOpenAI] = None

    @property
    def sync_client(self) -> OpenAI:
        """è·å–åŒæ­¥å®¢æˆ·ç«¯"""
        if self._sync_client is None:
            self._sync_client = OpenAI(
                api_key=self.api_key,
                base_url=self.api_base,
                timeout=self.timeout,
            )
        return self._sync_client

    @property
    def async_client(self) -> AsyncOpenAI:
        """è·å–å¼‚æ­¥å®¢æˆ·ç«¯"""
        if self._async_client is None:
            self._async_client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.api_base,
                timeout=self.timeout,
            )
        return self._async_client

    def _build_messages(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        history: Optional[List[Dict[str, str]]] = None,
    ) -> List[Dict[str, str]]:
        """æ„å»ºæ¶ˆæ¯åˆ—è¡¨

        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤º
            history: å†å²å¯¹è¯

        Returns:
            æ¶ˆæ¯åˆ—è¡¨
        """
        messages = []

        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        if history:
            messages.extend(history)

        messages.append({"role": "user", "content": prompt})

        return messages

    def _strip_thinking_tags(self, content: str) -> Tuple[str, Optional[str]]:
        """ä»å“åº”ä¸­æå–å¹¶ç§»é™¤ <think> æ ‡ç­¾å†…å®¹
        
        Qwen3 æ¨¡å‹åœ¨æ€è€ƒæ¨¡å¼ä¸‹ä¼šåœ¨ content ä¸­è¿”å› <think>...</think> æ ‡ç­¾åŒ…è£¹çš„æ€è€ƒå†…å®¹ã€‚
        æ­¤æ–¹æ³•å°†æ€è€ƒå†…å®¹æå–å‡ºæ¥ï¼Œå¹¶è¿”å›æ¸…ç†åçš„å†…å®¹ã€‚
        
        Args:
            content: åŸå§‹å“åº”å†…å®¹
            
        Returns:
            (cleaned_content, reasoning): æ¸…ç†åçš„å†…å®¹å’Œæå–çš„æ€è€ƒå†…å®¹
        """
        if not content:
            return content, None
        
        # åŒ¹é… <think>...</think> æ ‡ç­¾ï¼ˆæ”¯æŒå¤šè¡Œï¼‰
        think_pattern = re.compile(r'<think>(.*?)</think>', re.DOTALL)
        
        # æå–æ‰€æœ‰æ€è€ƒå†…å®¹
        think_matches = think_pattern.findall(content)
        reasoning = '\n'.join(match.strip() for match in think_matches) if think_matches else None
        
        # ç§»é™¤ <think> æ ‡ç­¾åŠå…¶å†…å®¹
        cleaned_content = think_pattern.sub('', content).strip()
        
        return cleaned_content, reasoning

    def _debug_print_request(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: Optional[int],
    ):
        """æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼šè¯·æ±‚"""
        print("\n" + "=" * 80)
        print("ğŸ”µ [DEBUG] LLM REQUEST")
        print("=" * 80)
        print(f"ğŸ“Œ Model: {model}")
        print(f"ğŸŒ¡ï¸  Temperature: {temperature}")
        print(f"ğŸ“Š Max Tokens: {max_tokens}")
        print("-" * 80)
        for i, msg in enumerate(messages):
            role = msg["role"].upper()
            content = msg["content"]
            print(f"\nğŸ“ [{role}] (Message {i+1})")
            print("-" * 40)
            print(content)
        print("\n" + "=" * 80 + "\n")

    def _debug_print_response(self, response: 'LLMResponse'):
        """æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼šå“åº”"""
        print("\n" + "=" * 80)
        print("ğŸŸ¢ [DEBUG] LLM RESPONSE")
        print("=" * 80)
        print(f"âœ… Status: {response.status}")
        print(f"â±ï¸  Time: {response.time_taken}s")
        if response.usage:
            print(f"ğŸ“Š Usage: {response.usage}")
        if response.reasoning:
            print("-" * 40)
            print("ğŸ§  REASONING:")
            print(response.reasoning)
        print("-" * 40)
        print("ğŸ’¬ CONTENT:")
        print(response.content)
        print("\n" + "=" * 80 + "\n")

    def call(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        history: Optional[List[Dict[str, str]]] = None,
        stream: bool = False,
    ) -> LLMResponse:
        """åŒæ­¥è°ƒç”¨ LLM

        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤º
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            history: å†å²å¯¹è¯
            stream: æ˜¯å¦æµå¼è¾“å‡º

        Returns:
            LLMResponse å¯¹è±¡
        """
        model = model or self.default_model
        temperature = temperature if temperature is not None else self.temperature
        max_tokens = max_tokens or self.max_tokens

        messages = self._build_messages(prompt, system_prompt, history)

        # Debug: æ‰“å°å®Œæ•´ prompt
        if self.debug:
            self._debug_print_request(model, messages, temperature, max_tokens)

        for attempt in range(self.max_retries):
            try:
                start_time = time.time()

                params = {
                    "model": model,
                    "messages": messages,
                    "temperature": temperature,
                    "stream": stream,
                }

                if max_tokens and max_tokens > 0:
                    params["max_tokens"] = max_tokens

                # Qwen3 æ€è€ƒæ¨¡å¼æ§åˆ¶ï¼ˆé€šè¿‡ extra_body ä¼ é€’ç»™ vLLM/SGLangï¼‰
                if self.enable_thinking is not None:
                    params["extra_body"] = {
                        "enable_thinking": self.enable_thinking}

                completion = self.sync_client.chat.completions.create(**params)

                if stream:
                    # æµå¼å¤„ç†
                    content = ""
                    reasoning = ""
                    for chunk in completion:
                        if hasattr(chunk.choices[0].delta, 'reasoning') and chunk.choices[0].delta.reasoning:
                            reasoning += chunk.choices[0].delta.reasoning
                        if chunk.choices[0].delta.content:
                            content += chunk.choices[0].delta.content

                    # å¤„ç† Qwen3 æ€è€ƒæ¨¡å¼çš„ <think> æ ‡ç­¾
                    cleaned_content, extracted_reasoning = self._strip_thinking_tags(content)
                    # ä¼˜å…ˆä½¿ç”¨ API è¿”å›çš„ reasoningï¼Œå…¶æ¬¡ä½¿ç”¨ä» <think> æ ‡ç­¾æå–çš„
                    final_reasoning = reasoning if reasoning else extracted_reasoning

                    response = LLMResponse(
                        status="success",
                        content=cleaned_content,
                        reasoning=final_reasoning if final_reasoning else None,
                        time_taken=round(time.time() - start_time, 2),
                    )
                else:
                    raw_content = completion.choices[0].message.content
                    api_reasoning = getattr(completion.choices[0].message, 'reasoning', None)
                    
                    # å¤„ç† Qwen3 æ€è€ƒæ¨¡å¼çš„ <think> æ ‡ç­¾
                    cleaned_content, extracted_reasoning = self._strip_thinking_tags(raw_content)
                    # ä¼˜å…ˆä½¿ç”¨ API è¿”å›çš„ reasoningï¼Œå…¶æ¬¡ä½¿ç”¨ä» <think> æ ‡ç­¾æå–çš„
                    final_reasoning = api_reasoning if api_reasoning else extracted_reasoning

                    response = LLMResponse(
                        status="success",
                        content=cleaned_content,
                        reasoning=final_reasoning,
                        usage=completion.usage.model_dump() if completion.usage else None,
                        time_taken=round(time.time() - start_time, 2),
                    )

                # Debug: æ‰“å°å®Œæ•´å“åº”
                if self.debug:
                    self._debug_print_response(response)

                return response

            except Exception as e:
                logger.warning(
                    f"LLM è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    return LLMResponse(
                        status="failed",
                        error=str(e),
                    )

    async def acall(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        history: Optional[List[Dict[str, str]]] = None,
    ) -> LLMResponse:
        """å¼‚æ­¥è°ƒç”¨ LLM

        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤º
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            history: å†å²å¯¹è¯

        Returns:
            LLMResponse å¯¹è±¡
        """
        model = model or self.default_model
        temperature = temperature if temperature is not None else self.temperature
        max_tokens = max_tokens or self.max_tokens

        messages = self._build_messages(prompt, system_prompt, history)

        # Debug: æ‰“å°å®Œæ•´ prompt
        if self.debug:
            self._debug_print_request(model, messages, temperature, max_tokens)

        for attempt in range(self.max_retries):
            try:
                start_time = time.time()

                params = {
                    "model": model,
                    "messages": messages,
                    "temperature": temperature,
                }

                if max_tokens and max_tokens > 0:
                    params["max_tokens"] = max_tokens

                # Qwen3 æ€è€ƒæ¨¡å¼æ§åˆ¶
                if self.enable_thinking is not None:
                    params["extra_body"] = {
                        "enable_thinking": self.enable_thinking}

                completion = await self.async_client.chat.completions.create(**params)

                raw_content = completion.choices[0].message.content
                api_reasoning = getattr(completion.choices[0].message, 'reasoning', None)
                
                # å¤„ç† Qwen3 æ€è€ƒæ¨¡å¼çš„ <think> æ ‡ç­¾
                cleaned_content, extracted_reasoning = self._strip_thinking_tags(raw_content)
                # ä¼˜å…ˆä½¿ç”¨ API è¿”å›çš„ reasoningï¼Œå…¶æ¬¡ä½¿ç”¨ä» <think> æ ‡ç­¾æå–çš„
                final_reasoning = api_reasoning if api_reasoning else extracted_reasoning

                response = LLMResponse(
                    status="success",
                    content=cleaned_content,
                    reasoning=final_reasoning,
                    usage=completion.usage.model_dump() if completion.usage else None,
                    time_taken=round(time.time() - start_time, 2),
                )

                # Debug: æ‰“å°å®Œæ•´å“åº”
                if self.debug:
                    self._debug_print_response(response)

                return response

            except Exception as e:
                logger.warning(
                    f"LLM å¼‚æ­¥è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return LLMResponse(
                        status="failed",
                        error=str(e),
                    )

    async def batch_call(
        self,
        prompts: List[str],
        system_prompt: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        max_concurrency: int = 5,
    ) -> List[LLMResponse]:
        """æ‰¹é‡å¼‚æ­¥è°ƒç”¨ LLM

        Args:
            prompts: æç¤ºåˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤º
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            max_concurrency: æœ€å¤§å¹¶å‘æ•°

        Returns:
            LLMResponse åˆ—è¡¨
        """
        semaphore = asyncio.Semaphore(max_concurrency)

        async def limited_call(prompt: str) -> LLMResponse:
            async with semaphore:
                return await self.acall(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )

        tasks = [limited_call(prompt) for prompt in prompts]
        return await asyncio.gather(*tasks)

    def call_with_retry(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        history: Optional[List[Dict[str, str]]] = None,
    ) -> str:
        """å¸¦é‡è¯•çš„åŒæ­¥è°ƒç”¨ï¼Œç›´æ¥è¿”å›å†…å®¹å­—ç¬¦ä¸²

        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤º
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            history: å†å²å¯¹è¯

        Returns:
            å“åº”å†…å®¹å­—ç¬¦ä¸²

        Raises:
            RuntimeError: å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        """
        response = self.call(
            prompt=prompt,
            system_prompt=system_prompt,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            history=history,
        )

        if response.status == "success":
            return response.content
        else:
            raise RuntimeError(f"LLM è°ƒç”¨å¤±è´¥: {response.error}")


# å…¨å±€å•ä¾‹
_llm_service: Optional[LLMService] = None


def get_llm_service(**kwargs) -> LLMService:
    """è·å– LLM æœåŠ¡å•ä¾‹

    Args:
        **kwargs: ä¼ é€’ç»™ LLMService çš„å‚æ•°

    Returns:
        LLMService å®ä¾‹
    """
    global _llm_service

    if _llm_service is None:
        _llm_service = LLMService(**kwargs)
    elif kwargs:
        # å¦‚æœå·²å­˜åœ¨å®ä¾‹ä½†ä¼ å…¥äº†å‚æ•°ï¼Œæ›´æ–°éƒ¨åˆ†å±æ€§
        for key, value in kwargs.items():
            if hasattr(_llm_service, key):
                setattr(_llm_service, key, value)

    return _llm_service


def set_debug_mode(enabled: bool = True):
    """è®¾ç½®å…¨å±€ LLM è°ƒè¯•æ¨¡å¼

    Args:
        enabled: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
    """
    service = get_llm_service()
    service.debug = enabled
    logger.info(f"LLM Debug æ¨¡å¼: {'å¯ç”¨' if enabled else 'ç¦ç”¨'}")
